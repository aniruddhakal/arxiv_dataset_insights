{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from logging import Logger, StreamHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fecb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../../dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e49fb11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arxiv-metadata-oai-snapshot.json',\n",
       " 'cache_dir',\n",
       " 'data',\n",
       " 'lemmatized_test_df_dataset_1.pq',\n",
       " 'lemmatized_test_df_dataset_2.pq',\n",
       " 'lemmatized_test_df_dataset_3.pq',\n",
       " 'lemmatized_test_df_dataset_4.pq',\n",
       " 'lemmatized_test_df_dataset_5.pq',\n",
       " 'lemmatized_train_df_dataset_1.pq',\n",
       " 'lemmatized_train_df_dataset_2.pq',\n",
       " 'lemmatized_train_df_dataset_3.pq',\n",
       " 'lemmatized_train_df_dataset_4.pq',\n",
       " 'lemmatized_train_df_dataset_5.pq',\n",
       " 'lemmatized_validation_df_dataset_1.pq',\n",
       " 'lemmatized_validation_df_dataset_2.pq',\n",
       " 'lemmatized_validation_df_dataset_3.pq',\n",
       " 'lemmatized_validation_df_dataset_4.pq',\n",
       " 'lemmatized_validation_df_dataset_5.pq',\n",
       " 'outliers_df.pq',\n",
       " 'parquet',\n",
       " 'split-test_dataset-1_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-test_dataset-1_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-test_dataset-1_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-4_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-validation_dataset-4_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-4_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-validation_dataset-5_model-allenai_scibert_scivocab_uncased_embeddings.npy',\n",
       " 'split-validation_dataset-5_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-validation_dataset-5_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-validation_dataset-5_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-5_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'test_df_dataset_1.pq',\n",
       " 'test_df_dataset_2.pq',\n",
       " 'test_df_dataset_3.pq',\n",
       " 'test_df_dataset_4.pq',\n",
       " 'test_df_dataset_5',\n",
       " 'test_df_dataset_5.pq',\n",
       " 'split-test_dataset-4_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-test_dataset-4_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-test_dataset-4_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-test_dataset-5_model-allenai_scibert_scivocab_uncased_embeddings.npy',\n",
       " 'split-test_dataset-5_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-1_model-allenai_scibert_scivocab_uncased_embeddings.npy',\n",
       " 'split-train_dataset-1_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-train_dataset-1_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-train_dataset-1_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-1_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-train_dataset-2_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-test_dataset-1_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-test_dataset-4_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-train_dataset-2_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-train_dataset-4_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-validation_dataset-2_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-validation_dataset-4_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'train_df_dataset_1.pq',\n",
       " 'split-train_dataset-4_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-train_dataset-4_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-4_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-train_dataset-5_model-allenai_scibert_scivocab_uncased_embeddings.npy',\n",
       " 'split-train_dataset-5_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-train_dataset-5_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-train_dataset-5_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-5_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-validation_dataset-1_model-allenai_scibert_scivocab_uncased_embeddings.npy',\n",
       " 'split-validation_dataset-1_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-validation_dataset-1_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-validation_dataset-1_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-1_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-validation_dataset-2_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-validation_dataset-2_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-2_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-validation_dataset-3_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-validation_dataset-3_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-validation_dataset-3_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-validation_dataset-3_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-train_dataset-2_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-2_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-train_dataset-3_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-train_dataset-3_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-train_dataset-3_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-train_dataset-3_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'train_df_dataset_2.pq',\n",
       " 'train_df_dataset_3.pq',\n",
       " 'train_df_dataset_4.pq',\n",
       " 'train_df_dataset_5',\n",
       " 'train_df_dataset_5.pq',\n",
       " 'validation_df_dataset_1.pq',\n",
       " 'validation_df_dataset_2.pq',\n",
       " 'validation_df_dataset_3.pq',\n",
       " 'validation_df_dataset_4.pq',\n",
       " 'validation_df_dataset_5',\n",
       " 'validation_df_dataset_5.pq',\n",
       " 'split-test_dataset-2_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-test_dataset-2_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-test_dataset-2_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-test_dataset-2_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy',\n",
       " 'split-test_dataset-3_model-distilbert-base-nli-mean-tokens_embeddings.npy',\n",
       " 'split-test_dataset-3_model-sentence-transformers_distilbert-base-nli-stsb-quora-ranking_embeddings.npy',\n",
       " 'split-test_dataset-3_model-sentence-transformers_distilroberta-base-paraphrase-v1_embeddings.npy',\n",
       " 'split-test_dataset-3_model-sentence-transformers_stsb-distilroberta-base-v2_embeddings.npy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f2887",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac74641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = 4\n",
    "model_name = \"sentence-transformers/distilroberta-base-paraphrase-v1\"\n",
    "splits = ['train', 'validation', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8ab46",
   "metadata": {},
   "source": [
    "### Load saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb8ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_embeddings(split_name: str, model_name: str):\n",
    "    embeddings_filename = get_embeddings_filename(split_name, model_name=model_name)\n",
    "\n",
    "    if Path(embeddings_filename).exists():\n",
    "        try:\n",
    "            embeddings = np.load(embeddings_filename)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Expected file named {embeddings_filename} was not found\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_filename(split_name, model_name):\n",
    "    model_normalized_name = re.sub(\"/\", \"_\", model_name)\n",
    "    \n",
    "    return str(\n",
    "        dataset_path /\n",
    "        f\"split-{split_name}_dataset-{dataset_index}_model-{model_normalized_name}_embeddings.npy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f208f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580762, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = load_embeddings(split_name='train', model_name=model_name)\n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc77a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225752, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_embeddings = load_embeddings(split_name='validation', model_name=model_name)\n",
    "validation_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150f000",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab5a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = dataset_path / 'cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc33820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_target_dataset(split: str, dataset_index: int = None):\n",
    "    prefix = \"\"\n",
    "\n",
    "    dataset = \\\n",
    "        load_dataset('parquet',\n",
    "                     data_files=[str(dataset_path / f\"{split}_df_dataset_{dataset_index}.pq\")],\n",
    "                     cache_dir=cache_dir)['train']\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b2bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/classifier/../../dataset/cache_dir/parquet/default-ff94db2645fa5e2d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34a2bbc30c44476b1c28bb4398925be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/classifier/../../dataset/cache_dir/parquet/default-cf285e0ca1bbb326/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d544034b474820ab401a33196cddec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/classifier/../../dataset/cache_dir/parquet/default-5433279a47d817c4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cacb7c74cb9470487ac968b0aa222d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_target_dataset(split='train', dataset_index=dataset_index)\n",
    "validation_dataset = load_target_dataset(split='validation', dataset_index=dataset_index)\n",
    "test_dataset = load_target_dataset(split='test', dataset_index=dataset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c489c62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451833, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings = load_embeddings(split_name='test', model_name=model_name)\n",
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34774301",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = train_dataset['categories_list']\n",
    "categories_list.extend(validation_dataset['categories_list'])\n",
    "categories_list.extend(test_dataset['categories_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b2c6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unique_categories = set()\n",
    "\n",
    "[all_unique_categories.update(x) for x in train_dataset['categories_list']]\n",
    "all_unique_categories = list(all_unique_categories)\n",
    "len(all_unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88df54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbc78e3c",
   "metadata": {},
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer(sparse_output=False)\n",
    "_ = multilabel_binarizer.fit_transform(train_dataset['categories_list'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2ba53fa",
   "metadata": {},
   "source": [
    "len(multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a9f167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path('../../models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c80a9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6769c14f",
   "metadata": {},
   "source": [
    "with open(models_dir / 'multilabel_binarizer.pkl', 'wb') as f:\n",
    "    pickle.dump(multilabel_binarizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814e773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(models_dir / 'multilabel_binarizer.pkl', 'rb') as f:\n",
    "    multilabel_binarizer = pickle.load(f)\n",
    "len(multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba093a",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5fc80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def transform_labels(labels: List[List[str]]):\n",
    "    y = multilabel_binarizer.transform(labels)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1acba41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580762, 176)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = transform_labels(train_dataset['categories_list'])\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8d9d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225752, 176)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y = transform_labels(validation_dataset['categories_list'])\n",
    "validation_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6193f6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451833, 176)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = transform_labels(test_dataset['categories_list'])\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "757710c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del train_dataset\n",
    "del test_dataset\n",
    "del validation_dataset\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad343b",
   "metadata": {},
   "source": [
    "### Build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838d8038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abbf53b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = train_embeddings.shape[1]\n",
    "out_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318875d5",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb90e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ArxivDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2352b8a1",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "class ArxivAbstractClassifier(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ArxivAbstractClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51d713b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 03:03:21.990947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 03:03:22.696305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "class ArxivAbstractClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ArxivAbstractClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.BCELoss()(outputs, targets)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "#         return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e54dcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset instance\n",
    "train_dataset = ArxivDataset(train_embeddings, train_y.astype(np.float32))\n",
    "validation_dataset = ArxivDataset(validation_embeddings, validation_y.astype(np.float32))\n",
    "test_dataset = ArxivDataset(test_embeddings, test_y.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0d42214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = train_y.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "300439b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArxivAbstractClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=128, out_features=176, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ArxivAbstractClassifier(input_size=768, num_classes=num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "066d30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f283d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import Logger, StreamHandler\n",
    "logger = Logger(__name__)\n",
    "logger.addHandler(StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "def train(train_dataloader: DataLoader, validation_dataloader: DataLoader):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for X, y in tqdm(train_dataloader):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd720bf",
   "metadata": {},
   "source": [
    "### Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c65cc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 768\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61aa844b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | fc1     | Linear  | 393 K \n",
      "1 | relu1   | ReLU    | 0     \n",
      "2 | fc2     | Linear  | 131 K \n",
      "3 | relu2   | ReLU    | 0     \n",
      "4 | fc3     | Linear  | 32.9 K\n",
      "5 | relu3   | ReLU    | 0     \n",
      "6 | fc4     | Linear  | 22.7 K\n",
      "7 | sigmoid | Sigmoid | 0     \n",
      "------------------------------------\n",
      "580 K     Trainable params\n",
      "0         Non-trainable params\n",
      "580 K     Total params\n",
      "2.323     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5c677f20447ed939e32b9013d75cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_cpus = max(os.cpu_count() - 3, 1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the model\n",
    "input_size = 768\n",
    "model = ArxivAbstractClassifier(input_size, num_classes)\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5c24d",
   "metadata": {},
   "source": [
    "### Evaluate a classifier on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9c064ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.test(model=model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f986d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
