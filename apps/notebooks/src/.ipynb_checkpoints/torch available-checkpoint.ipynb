{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063a734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b3acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c25c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34283570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:21000'], document_class=dict, tz_aware=False, connect=True), 'arxiv-db'), 'arxiv-dataset-collection')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_name = \"arxiv-db\"\n",
    "collection_name = \"arxiv-dataset-collection\"\n",
    "\n",
    "db = pymongo.MongoClient(host=\"localhost\", port=21000).get_database(db_name)\n",
    "collection = db[collection_name]\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fcd8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cursor):\n",
    "    all_data_df = pd.DataFrame(cursor)\n",
    "    return all_data_df\n",
    "\n",
    "def load_data_cursor(batch_size=128):\n",
    "    cursor = collection.find({}, {\"title\", \"abstract\", \"categories\"}).batch_size(batch_size)\n",
    "    return cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fd6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x7f8990e7dcd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = load_data_cursor()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddeb23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cursor:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbdb0fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('64729c935ced617335d85d64'),\n",
       " 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       " 'categories': 'hep-ph',\n",
       " 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and preprocess the ArXiv dataset\n",
    "# # Replace this with your own dataset loading and preprocessing code\n",
    "# all_data_df = load_data()\n",
    "# all_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f758a012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def extract_embeddings(texts, model, tokenizer, device='cuda'):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    embeddings = outputs['pooler_output'].detach().cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34f008a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adbc82960a04a809681a551ae71f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917a3bad4e404cc9b46da982d63c6fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33acdf7ff334420c81c199998470087c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e38d7f74f7248a8974fcecc4909cf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353e069a747c49bf9ffd2d71564fdd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set up the BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with any other BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = extract_embeddings(texts=all_data_df['abstract'].tolist(), model=model, tokenizer=tokenizer)\n",
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19224a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up the Sentence-BERT model\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Extract embeddings using SBERT\n",
    "embeddings = extract_embeddings(abstracts, sbert_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import DBSCAN, HDBSCAN, AgglomerativeClustering\n",
    "\n",
    "\n",
    "def perform_gpu_dbscan(embeddings, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(embeddings)\n",
    "    labels = dbscan.labels_\n",
    "    return labels\n",
    "\n",
    "\n",
    "def perform_gpu_hdbscan(embeddings, min_cluster_size=5, min_samples=5):\n",
    "    hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "    hdbscan.fit(embeddings)\n",
    "    labels = hdbscan.labels_\n",
    "    return labels\n",
    "\n",
    "\n",
    "def perform_gpu_agglomerative_clustering(embeddings, n_clusters):\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def find_optimal_clusters(embeddings, clustering_algorithm):\n",
    "    scores = []\n",
    "    max_clusters = min(10, len(embeddings))\n",
    "\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        labels = clustering_algorithm(embeddings, n_clusters)\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    optimal_clusters = np.argmax(scores) + 2\n",
    "    return optimal_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GPU-accelerated DBSCAN\n",
    "gpu_dbscan_labels = perform_gpu_dbscan(embeddings, eps=0.5, min_samples=5)\n",
    "\n",
    "# Perform GPU-accelerated HDBSCAN\n",
    "gpu_hdbscan_labels = perform_gpu_hdbscan(embeddings, min_cluster_size=5, min_samples=5)\n",
    "\n",
    "# Perform GPU-accelerated Agglomerative Clustering\n",
    "gpu_agglomerative_labels = perform_gpu_agglomerative_clustering(embeddings, n_clusters=5)\n",
    "\n",
    "# Find the optimal number of clusters for each algorithm\n",
    "optimal_dbscan_clusters = find_optimal_clusters(embeddings, perform_gpu_dbscan)\n",
    "optimal_hdbscan_clusters = find_optimal_clusters(embeddings, perform_gpu_hdbscan)\n",
    "optimal_agglomerative_clusters = find_optimal_clusters(embeddings, perform_gpu_agglomerative_clustering)\n",
    "\n",
    "# Print the results\n",
    "print(\"GPU-accelerated DBSCAN Clustering:\")\n",
    "print(\"Optimal number of clusters:\", optimal_dbscan_clusters)\n",
    "print(\"Cluster labels:\", gpu_dbscan_labels)\n",
    "\n",
    "print(\"\\nGPU-accelerated HDBSCAN Clustering:\")\n",
    "print(\"Optimal number of clusters:\", optimal_hdbscan_clusters)\n",
    "print(\"Cluster labels:\", gpu_hdbscan_labels)\n",
    "\n",
    "print(\"\\nGPU-accelerated Agglomerative Clustering:\")\n",
    "print(\"Optimal number of clusters:\", optimal_agglomerative_clusters)\n",
    "print(\"Cluster labels:\", gpu_agglomerative_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
