{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98b8494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e1a9f",
   "metadata": {},
   "source": [
    "##### References\n",
    "- https://www.kaggle.com/code/maartengr/topic-modeling-arxiv-abstract-with-bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cdf66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc30cb2",
   "metadata": {},
   "source": [
    "### TODO visit back on list of embedding models\n",
    "- paraphrase-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80b540",
   "metadata": {},
   "source": [
    "### TODO finalize few appropriate evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141af8",
   "metadata": {},
   "source": [
    "### TODO select train test validation dataset, and export it to pq, then load using hf datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68876e73",
   "metadata": {},
   "source": [
    "### TODO identify hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763db209",
   "metadata": {},
   "source": [
    "### TODO setup hyperparam tuning study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347df14f",
   "metadata": {},
   "source": [
    "### TODO perform hyperparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b83980",
   "metadata": {},
   "source": [
    "### TODO apply dim reduction on embeddings, and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9ea97",
   "metadata": {},
   "source": [
    "### TODO start preparing comprehensive report - ReadMe.md file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7aa4e",
   "metadata": {},
   "source": [
    "### TODO create an inference service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea2035",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2809caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"../../../dataset/\")\n",
    "cache_dir = dataset_path / \"cache_dir\"\n",
    "\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fe09d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arxiv-metadata-oai-snapshot.json',\n",
       " 'cache_dir',\n",
       " 'data',\n",
       " 'test_df_dataset_1.pq',\n",
       " 'test_df_dataset_2.pq',\n",
       " 'test_df_dataset_3.pq',\n",
       " 'test_df_dataset_4.pq',\n",
       " 'train_df_dataset_1.pq',\n",
       " 'train_df_dataset_2.pq',\n",
       " 'train_df_dataset_3.pq',\n",
       " 'train_df_dataset_4.pq',\n",
       " 'validation_df_dataset_1.pq',\n",
       " 'validation_df_dataset_2.pq',\n",
       " 'validation_df_dataset_3.pq',\n",
       " 'validation_df_dataset_4.pq']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96210bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_datasets(dataset_index: int):\n",
    "    train_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"train_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    validation_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"validation_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    test_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"test_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    \n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd2543c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/notebooks/src/../../../dataset/cache_dir/parquet/default-ac9b42d7382cd426/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f4552045454d80b4c453084e4843b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to /mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/notebooks/src/../../../dataset/cache_dir/parquet/default-8dca40a1fc3c3914/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7569086579644fedac1f148fb784eea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fd1aa94487494192e15873ac57cb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/notebooks/src/../../../dataset/cache_dir/parquet/default-8dca40a1fc3c3914/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a54b19a4e864c93bcaf149ac310b930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to /mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/notebooks/src/../../../dataset/cache_dir/parquet/default-caa8a136896649d1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f11e0f5ad421c89eaf680b17bf004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1d4a5ba16f4eb58f3eb9b467e18d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /mnt/NVMe/workspace/github_projects/arxiv_dataset_insights/apps/notebooks/src/../../../dataset/cache_dir/parquet/default-caa8a136896649d1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b6cbcf3c4c40358caeb8b13ff373af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_index = 1\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = load_datasets(dataset_index=dataset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8b8aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c340c",
   "metadata": {},
   "source": [
    "### TODO select best embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638df024",
   "metadata": {},
   "source": [
    "### Extract Embeddings using Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e56b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_trf_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "# sentence_trf_model = 'distilbert-base-nli-mean-tokens'\n",
    "\n",
    "batch_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc2d8c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65d34b6e1004626b7f3921d8d89d75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = train_dataset['abstract'][:10000]\n",
    "sentence_transformer = SentenceTransformer(model_name_or_path=sentence_trf_model, device=device)\n",
    "embeddings = sentence_transformer.encode(sentences=sentences,\n",
    "                                         batch_size=batch_size,\n",
    "                                         device=device,\n",
    "                                         convert_to_numpy=True,\n",
    "                                         show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad22b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc422c",
   "metadata": {},
   "source": [
    "### Train Model using hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "nr_topics = 30\n",
    "top_n_words = 100\n",
    "min_topic_size = 10\n",
    "n_gram_range = (1, 1)\n",
    "\n",
    "# TODO inputs for hyperparameters\n",
    "min_categories = 5\n",
    "max_categories = 5\n",
    "\n",
    "# count vectorizer params\n",
    "max_features = 100\n",
    "max_df=0.8\n",
    "min_df=0.05\n",
    "ngram_range=(1,1),\n",
    "lowercase=True\n",
    "stop_words=STOPWORDS\n",
    "\n",
    "# metrics params\n",
    "topk=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbfc92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "\n",
    "best_num_categories = min_categories\n",
    "best_silhouette_score = -1.0\n",
    "\n",
    "scores = {\n",
    "    'coherence': [],\n",
    "    'diversity': [],\n",
    "}\n",
    "\n",
    "\n",
    "def append_score(metric_name: str, score):\n",
    "    scores_list = scores.get(metric_name)\n",
    "    \n",
    "    if scores_list is None:\n",
    "        raise Exception(\"Invalid scoring metric\")\n",
    "        \n",
    "    scores_list.append(score)\n",
    "    scores[metric_name] = scores_list\n",
    "\n",
    "\n",
    "for num_categories in range(min_categories, max_categories + 1):\n",
    "    count_vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df,\n",
    "                                       ngram_range=ngram_range, lowercase=lowercase, stop_words=list(STOPWORDS))\n",
    "    \n",
    "    # Create BERTopic with current number of categories\n",
    "    model = BERTopic(\n",
    "                nr_topics=num_categories,\n",
    "                vectorizer_model=count_vectorizer,\n",
    "                n_gram_range=ngram_range\n",
    "            )\n",
    "    \n",
    "    topics, probabilities = model.fit_transform(sentences)\n",
    "    \n",
    "    #--------------------------------\n",
    "    # for calculating coherence score\n",
    "    cleaned_docs = model._preprocess_text(sentences)\n",
    "    analyzer = model.vectorizer_model.build_analyzer()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    \n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    \n",
    "    topics = model.get_topics()\n",
    "    topics.pop(-1, None)\n",
    "    \n",
    "    topic_words = [\n",
    "        [\n",
    "            words for words, _ in model.get_topic(topic)\n",
    "        ] \n",
    "        for topic in range(len(set(topics))-1)\n",
    "    ]\n",
    "    #--------------------------------\n",
    "    \n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                              texts=tokens, \n",
    "                              corpus=corpus,\n",
    "                              dictionary=dictionary, \n",
    "                              coherence='c_v')\n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    append_score(metric_name='coherence', score=coherence_score)\n",
    "    \n",
    "    # Metric 1 - topic coherence\n",
    "    # Metric 2 - topic diversity\n",
    "    # Calculate Coherence & Diversity score\n",
    "    # ----------------------\n",
    "#     topic_words = model.get_topic_freq().index.to_list()\n",
    "#     vocab_dict = count_vectorizer.vocabulary_\n",
    "#     topic_words_list = list(vocab_dict.keys())\n",
    "    \n",
    "#     dictionary = Dictionary(topic_words_list)\n",
    "#     corpus = [dictionary.doc2bow(word) for word in topic_words_list]\n",
    "    \n",
    "    \n",
    "#     bertopic_topics = [\n",
    "#         [\n",
    "#             vals[0] if vals[0] in all_words else all_words[0]\n",
    "#             for vals in model.get_topic(i)[:topk]\n",
    "#         ]\n",
    "#         for i in range(len(set(topics)) - 1)\n",
    "#     ]\n",
    "\n",
    "#     output_tm = {\"topics\": bertopic_topics}\n",
    "    \n",
    "#     npmi = Coherence(texts=sentences, topk=topk, measure=\"c_npmi\")\n",
    "#     npmi_score = npmi.score(model_output=output_tm)\n",
    "#     append_score(metric_name='coherence', score=npmi_score)\n",
    "    \n",
    "#     topic_diversity = TopicDiversity(topk=self.topk)\n",
    "#     diversity_score = topic_diversity.score(model_output=output_tm)\n",
    "#     append_score(metric_name='diversity', score=diversity_score)\n",
    "    \n",
    "    \n",
    "#     coherence_model = CoherenceModel(topics=model.get_topics(), texts=sentences)\n",
    "#     coherence_model = CoherenceModel(topics=model.get_topics(), dictionary=dictionary)\n",
    "    \n",
    "#     coherence_score = coherence_model.get_coherence()\n",
    "#     append_score(metric_name='coherence', score=coherence_score)\n",
    "\n",
    "#     # Calculate Topic Diversity\n",
    "#     topic_diversity = model.calculate_topic_diversity()\n",
    "#     append_score(metric_name='diversity', score=topic_diversity)\n",
    "#     # ----------------------\n",
    "    \n",
    "    # TODO Metric 4 - Combine all 3 metrics into one and prepare one objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97149ec",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bac2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
