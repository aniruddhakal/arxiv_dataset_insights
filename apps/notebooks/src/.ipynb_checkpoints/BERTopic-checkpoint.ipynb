{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e1a9f",
   "metadata": {},
   "source": [
    "##### References\n",
    "- https://www.kaggle.com/code/maartengr/topic-modeling-arxiv-abstract-with-bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc30cb2",
   "metadata": {},
   "source": [
    "### TODO visit back on list of embedding models\n",
    "- paraphrase-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80b540",
   "metadata": {},
   "source": [
    "### TODO finalize few appropriate evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141af8",
   "metadata": {},
   "source": [
    "### TODO select train test validation dataset, and export it to pq, then load using hf datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68876e73",
   "metadata": {},
   "source": [
    "### TODO identify hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763db209",
   "metadata": {},
   "source": [
    "### TODO setup hyperparam tuning study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347df14f",
   "metadata": {},
   "source": [
    "### TODO perform hyperparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b83980",
   "metadata": {},
   "source": [
    "### TODO apply dim reduction on embeddings, and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9ea97",
   "metadata": {},
   "source": [
    "### TODO start preparing comprehensive report - ReadMe.md file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7aa4e",
   "metadata": {},
   "source": [
    "### TODO create an inference service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea2035",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"../../../dataset/\")\n",
    "cache_dir = dataset_path / \"cache_dir\"\n",
    "\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe09d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96210bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_datasets(dataset_index: int):\n",
    "    train_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"train_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    validation_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"validation_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    test_dataset = load_dataset('parquet', data_files=[str(dataset_path / f\"test_df_dataset_{dataset_index}.pq\")], cache_dir=cache_dir)['train']\n",
    "    \n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2543c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_index = 1\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = load_datasets(dataset_index=dataset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c340c",
   "metadata": {},
   "source": [
    "### TODO select best embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638df024",
   "metadata": {},
   "source": [
    "### Extract Embeddings using Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e56b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "\n",
    "batch_size = 384"
   ]
  },
  {
   "cell_type": "raw",
   "id": "372a8e39",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = train_dataset['abstract'][:10000]\n",
    "sentence_transformer = SentenceTransformer(model_name_or_path=model_name, device=device)\n",
    "embeddings = sentence_transformer.encode(sentences=sentences,\n",
    "                                         batch_size=batch_size,\n",
    "                                         device=device,\n",
    "                                         convert_to_numpy=True,\n",
    "                                         show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_filename(split_name):\n",
    "    return str(\n",
    "        dataset_path /\n",
    "        f\"split-{split_name}_dataset-{dataset_index}_model-{model_name}_embeddings.npy\"\n",
    "    )\n",
    "\n",
    "def load_embeddings(split_name: str):\n",
    "    embeddings_filename = get_embeddings_filename(split_name)\n",
    "    embeddings = None\n",
    "\n",
    "    if Path(embeddings_filename).exists():\n",
    "        try:\n",
    "            embeddings = np.load(embeddings_filename)\n",
    "        except FileNotFoundError as e:\n",
    "            self.logger.error(f\"Expected file named {embeddings_filename} was not found\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50239950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad22b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = load_embeddings('train')\n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_embeddings = load_embeddings('validation')\n",
    "validation_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8863d0",
   "metadata": {},
   "source": [
    "### Dim Reduction, Clsutering and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0b1a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cuml import UMAP\n",
    "\n",
    "umap = UMAP(n_neighbors=15, n_components=3, metric='euclidean', n_epochs=200, learning_rate=1.0, min_dist=0.1,\n",
    "           random_state=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 30000\n",
    "select = 5000\n",
    "select_embeddings = None\n",
    "\n",
    "for i in range(0, len(train_embeddings) + step, step):\n",
    "    append_new = train_embeddings[i:i+select]\n",
    "    \n",
    "    if select_embeddings is None:\n",
    "        select_embeddings = append_new\n",
    "    else:\n",
    "        select_embeddings = np.concatenate((select_embeddings, append_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ac0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_transformed = umap.fit_transform(train_embeddings[:])\n",
    "train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cf7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter_3d(data_frame=train_transformed, x=0, y=1, z=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "331c55be",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "select_transformed = umap.fit_transform(select_embeddings[:])\n",
    "select_transformed.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7d1b140",
   "metadata": {},
   "source": [
    "px.scatter_3d(data_frame=select_transformed, x=0, y=1, z=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbfac9",
   "metadata": {},
   "source": [
    "### With Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54501316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml.cluster.hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=0, cluster_selection_epsilon=0.1, metric='euclidean', cluster_selection_method='eom')\n",
    "hdbscan_pred = hdbscan.fit_predict(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37afd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(hdbscan_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fde182",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(data_frame=train_transformed, x=0, y=1, z=2, color=hdbscan_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc422c",
   "metadata": {},
   "source": [
    "### Train Model using hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433620f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# hyperparameters\n",
    "nr_topics = 30\n",
    "top_n_words = 100\n",
    "min_topic_size = 10\n",
    "n_gram_range = (1, 1)\n",
    "\n",
    "# TODO inputs for hyperparameters\n",
    "min_categories = 5\n",
    "max_categories = 5\n",
    "\n",
    "# count vectorizer params\n",
    "max_features = 100\n",
    "max_df=0.8\n",
    "min_df=0.05\n",
    "ngram_range=(1,1),\n",
    "lowercase=True\n",
    "stop_words=STOPWORDS\n",
    "\n",
    "# metrics params\n",
    "topk=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbfc92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "best_num_categories = min_categories\n",
    "best_silhouette_score = -1.0\n",
    "\n",
    "scores = {\n",
    "    'coherence': [],\n",
    "    'diversity': [],\n",
    "}\n",
    "\n",
    "\n",
    "def append_score(metric_name: str, score):\n",
    "    scores_list = scores.get(metric_name)\n",
    "    \n",
    "    if scores_list is None:\n",
    "        raise Exception(\"Invalid scoring metric\")\n",
    "        \n",
    "    scores_list.append(score)\n",
    "    scores[metric_name] = scores_list\n",
    "\n",
    "\n",
    "for num_categories in range(min_categories, max_categories + 1):\n",
    "    count_vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df,\n",
    "                                       ngram_range=ngram_range, lowercase=lowercase, stop_words=list(STOPWORDS))\n",
    "    \n",
    "    # Create BERTopic with current number of categories\n",
    "    model = BERTopic(\n",
    "                nr_topics=num_categories,\n",
    "                vectorizer_model=count_vectorizer,\n",
    "                n_gram_range=ngram_range\n",
    "            )\n",
    "    \n",
    "    topics, probabilities = model.fit_transform(sentences)\n",
    "    \n",
    "    #--------------------------------\n",
    "    # for calculating coherence score\n",
    "    cleaned_docs = model._preprocess_text(sentences)\n",
    "    analyzer = model.vectorizer_model.build_analyzer()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    \n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    \n",
    "    topics = model.get_topics()\n",
    "    topics.pop(-1, None)\n",
    "    \n",
    "    topic_words = [\n",
    "        [\n",
    "            words for words, _ in model.get_topic(topic)\n",
    "        ] \n",
    "        for topic in range(len(set(topics))-1)\n",
    "    ]\n",
    "    #--------------------------------\n",
    "    \n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                              texts=tokens, \n",
    "                              corpus=corpus,\n",
    "                              dictionary=dictionary, \n",
    "                              coherence='c_v')\n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    append_score(metric_name='coherence', score=coherence_score)\n",
    "    \n",
    "    # Metric 1 - topic coherence\n",
    "    # Metric 2 - topic diversity\n",
    "    # Calculate Coherence & Diversity score\n",
    "    # ----------------------\n",
    "#     topic_words = model.get_topic_freq().index.to_list()\n",
    "#     vocab_dict = count_vectorizer.vocabulary_\n",
    "#     topic_words_list = list(vocab_dict.keys())\n",
    "    \n",
    "#     dictionary = Dictionary(topic_words_list)\n",
    "#     corpus = [dictionary.doc2bow(word) for word in topic_words_list]\n",
    "    \n",
    "    \n",
    "#     bertopic_topics = [\n",
    "#         [\n",
    "#             vals[0] if vals[0] in all_words else all_words[0]\n",
    "#             for vals in model.get_topic(i)[:topk]\n",
    "#         ]\n",
    "#         for i in range(len(set(topics)) - 1)\n",
    "#     ]\n",
    "\n",
    "#     output_tm = {\"topics\": bertopic_topics}\n",
    "    \n",
    "#     npmi = Coherence(texts=sentences, topk=topk, measure=\"c_npmi\")\n",
    "#     npmi_score = npmi.score(model_output=output_tm)\n",
    "#     append_score(metric_name='coherence', score=npmi_score)\n",
    "    \n",
    "#     topic_diversity = TopicDiversity(topk=self.topk)\n",
    "#     diversity_score = topic_diversity.score(model_output=output_tm)\n",
    "#     append_score(metric_name='diversity', score=diversity_score)\n",
    "    \n",
    "    \n",
    "#     coherence_model = CoherenceModel(topics=model.get_topics(), texts=sentences)\n",
    "#     coherence_model = CoherenceModel(topics=model.get_topics(), dictionary=dictionary)\n",
    "    \n",
    "#     coherence_score = coherence_model.get_coherence()\n",
    "#     append_score(metric_name='coherence', score=coherence_score)\n",
    "\n",
    "#     # Calculate Topic Diversity\n",
    "#     topic_diversity = model.calculate_topic_diversity()\n",
    "#     append_score(metric_name='diversity', score=topic_diversity)\n",
    "#     # ----------------------\n",
    "    \n",
    "    # TODO Metric 4 - Combine all 3 metrics into one and prepare one objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97149ec",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bac2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
