logging_dir: "../logs"
logfile_name: "bertopic_trainer_logs.txt"
logging_level: "DEBUG"

study_name: "distilled_nli_models"
study_storage_filename: "distilled_model_choice_experiments_dataset5.sql"

dataset_path: "../../dataset"
models_path: "../../models"

batch_size: 384
dataset_index: 1
n_trials: 5

hyperparameters:
  model_name: [
    'sentence-transformers/distilroberta-base-paraphrase-v1',
    'sentence-transformers/stsb-distilroberta-base-v2',
    'sentence-transformers/distilbert-base-nli-stsb-quora-ranking',
    'distilbert-base-nli-mean-tokens'
  ]

  nr_topics: [5, 15, 25, 45, 75, 95, 135, 160, 190, 200, 210, 215, 300, 400]
  top_n_words: [10, 20, 30, 60, 80, 100]
  min_topic_size: [1, 5, 15, 30, 50, 100]
  #  n_gram_range: [(1, 1), (1,2), (1,3)]
  n_gram_range_start: [ 1 ]
  n_gram_range_end: [ 1, 2, 3 ]
  
  # count vectorizer params
  max_features: [500, 5000, 10000, 20000, 30000, 60000]
  max_df: [0.75, 0.8, 0.86, 0.999]
  min_df: [0.001, 0.01, 0.05, 0.1, 0.13]
  lowercase: [true, false]

  # umap params
  n_neighbors: [3, 5, 7, 15, 25, 40]
  n_components: [64, 128, 256]
  umap_metric: ['euclidean']
  n_epochs: [200]
  learning_rate: [1.0]
  min_dist: [0.1]
  random_state: [65]

  # hdbscan params
  min_cluster_size: [1, 5, 15, 30, 50, 100]
  cluster_selection_epsilon: [0.001, 0.01, 0.1, 0.2, 0.3, 0.4]
  hdbscan_metric:  ['euclidean']
  cluster_selection_method: ['eom']

  # metrics params
  topk: [10]