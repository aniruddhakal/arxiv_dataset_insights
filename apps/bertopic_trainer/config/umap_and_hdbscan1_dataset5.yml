logging_dir: "../logs"
logfile_name: "bertopic_trainer_logs.txt"
logging_level: "DEBUG"

study_name: "umap_and_hdbscan1_dataset5"
study_storage_filename: "distilled_model_choice_experiments_dataset5.sql"

dataset_path: "../../dataset"
models_path: "../../models"

batch_size: 3072
dataset_index: 5
n_trials: 18816

hyperparameters:
  model_name: [
    'sentence-transformers/distilroberta-base-paraphrase-v1',
  ]

  nr_topics: [10, 12, 15, 340, 800, 1200, 1500]
  top_n_words: [80]
  min_topic_size: [30]
  n_gram_range_start: [ 1 ]
  n_gram_range_end: [ 1 ]
  
  # count vectorizer params
  max_features: [30000]
  max_df: [0.93]
  min_df: [0.1]
  lowercase: [true]

  # umap params
  n_neighbors: [45]
  n_components: [ 128, 256, 512, 640 ]
  umap_metric: ['euclidean']
  n_epochs: [200]
  learning_rate: [1.0]
  min_dist: [0.01, 0.1, 0.2, 0.3]
  random_state: [65]

  # hdbscan params
  min_cluster_size: [ 800, 950, 1000, 1200, 1500, 1800]
  cluster_selection_epsilon: [0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]
  hdbscan_metric:  ['euclidean']
  cluster_selection_method: ['eom', 'leaf']

  # metrics params
  topk: [10]