{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f02a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "dataset_path = Path(\"../../dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23fabee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemmatized_test_df_dataset_1.pq',\n",
       " 'lemmatized_train_df_dataset_1.pq',\n",
       " 'lemmatized_validation_df_dataset_1.pq',\n",
       " 'outliers_df.pq',\n",
       " 'test_df_dataset_1.pq',\n",
       " 'test_df_dataset_2.pq',\n",
       " 'test_df_dataset_3.pq',\n",
       " 'test_df_dataset_4.pq',\n",
       " 'test_df_dataset_5.pq',\n",
       " 'train_df_dataset_1.pq',\n",
       " 'train_df_dataset_2.pq',\n",
       " 'train_df_dataset_3.pq',\n",
       " 'train_df_dataset_4.pq',\n",
       " 'train_df_dataset_5.pq',\n",
       " 'validation_df_dataset_1.pq',\n",
       " 'validation_df_dataset_2.pq',\n",
       " 'validation_df_dataset_3.pq',\n",
       " 'validation_df_dataset_4.pq',\n",
       " 'validation_df_dataset_5.pq']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in os.listdir(dataset_path) if x.endswith('.pq')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d39346c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/cupy/_environment.py:445: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda11x, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c6afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'test', 'validation']\n",
    "dataset_indices = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7bd321f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "split = 'validation'\n",
    "dataset_index = 1\n",
    "df = pd.read_parquet(dataset_path / f\"{split}_df_dataset_{dataset_index}.pq\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de7bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from logging import Logger, StreamHandler\n",
    "\n",
    "logger = Logger(\"dummy\")\n",
    "logger.addHandler(StreamHandler())\n",
    "\n",
    "n_workers = max(1, os.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b33b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset - 1\n",
      "Processing split train\n",
      "Read df of shape (903265, 6)\n",
      "Finished Processing df\n",
      "Saved processed df to ../../dataset/lemmatized_train_df_dataset_1.pq\n",
      "Processing split test\n",
      "Read df of shape (903489, 6)\n",
      "Finished Processing df\n",
      "Saved processed df to ../../dataset/lemmatized_test_df_dataset_1.pq\n",
      "Processing split validation\n",
      "Read df of shape (451593, 6)\n",
      "Finished Processing df\n",
      "Saved processed df to ../../dataset/lemmatized_validation_df_dataset_1.pq\n",
      "Processing dataset - 2\n",
      "Processing split train\n",
      "Read df of shape (1354937, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead df of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatize_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Processing df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mlemmatize_sentences\u001b[0;34m(sentences, join)\u001b[0m\n\u001b[1;32m     15\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sentence \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences] \u001b[38;5;66;03m# TODO just check 0th element, no need to check everything\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe(sentences, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_process\u001b[38;5;241m=\u001b[39mn_workers):\n\u001b[1;32m     18\u001b[0m     lemmatized_sentence \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "File \u001b[0;32m/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/spacy/language.py:1574\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/spacy/language.py:1648\u001b[0m, in \u001b[0;36mLanguage._multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (_, (byte_doc, context, byte_error)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1649\u001b[0m         \u001b[38;5;28mzip\u001b[39m(raw_texts, byte_tuples), \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1650\u001b[0m     ):\n\u001b[1;32m   1651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m byte_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/NVMe2/VirtualEnvironments/general_python_venv/lib/python3.8/site-packages/spacy/language.py:1645\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Cycle channels not to break the order of docs.\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;66;03m# The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m byte_tuples \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 1645\u001b[0m     \u001b[43mrecv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m recv \u001b[38;5;129;01min\u001b[39;00m cycle(bytedocs_recv_ch)\n\u001b[1;32m   1646\u001b[0m )\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:251\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recv_bytes()\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 384\n",
    "\n",
    "def lemmatize_sentences(sentences, join=True):\n",
    "    lemmatized_sentences = []\n",
    "\n",
    "    sentences = [' '.join(sentence) if isinstance(sentence, list) else sentence for sentence in sentences] # TODO just check 0th element, no need to check everything\n",
    "\n",
    "    for doc in nlp.pipe(sentences, batch_size=batch_size, disable=[\"parser\", \"ner\"], n_process=n_workers):\n",
    "        lemmatized_sentence = [token.lemma_ for token in doc]\n",
    "\n",
    "        if join:\n",
    "            lemmatized_sentence = ' '.join(lemmatized_sentence)\n",
    "\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentences\n",
    "\n",
    "\n",
    "for dataset_index in [1]:\n",
    "    logger.info(f\"Processing dataset - {dataset_index}\")\n",
    "    \n",
    "    for split in splits:\n",
    "        logger.info(f\"Processing split {split}\")\n",
    "        \n",
    "        df = pd.read_parquet(dataset_path / f\"{split}_df_dataset_{dataset_index}.pq\")\n",
    "        logger.info(f\"Read df of shape {df.shape}\")\n",
    "        \n",
    "        df['abstract'] = lemmatize_sentences(sentences=df['abstract'].tolist())\n",
    "        logger.info(f\"Finished Processing df\")\n",
    "        \n",
    "        target_name = dataset_path / f\"lemmatized_{split}_df_dataset_{dataset_index}.pq\"\n",
    "        df.to_parquet(target_name)\n",
    "        logger.info(f\"Saved processed df to {target_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb5765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2258347, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_parquet(dataset_path/'lemmatized_test_df_dataset_1.pq')\n",
    "df2 = pd.read_parquet(dataset_path/'lemmatized_train_df_dataset_1.pq')\n",
    "df3 = pd.read_parquet(dataset_path/'lemmatized_validation_df_dataset_1.pq')\n",
    "df = pd.concat([df1, df2, df3])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb66975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 2\n",
      "Processing split train\n",
      "source shape: (1354937, 6)\n",
      "new source shape: (1354937, 6)\n",
      "Processing split test\n",
      "source shape: (677658, 6)\n",
      "new source shape: (677658, 6)\n",
      "Processing split validation\n",
      "source shape: (225752, 6)\n",
      "new source shape: (225752, 6)\n",
      "Processing index: 3\n",
      "Processing split train\n",
      "source shape: (1354937, 6)\n",
      "new source shape: (1354937, 6)\n",
      "Processing split test\n",
      "source shape: (451817, 6)\n",
      "new source shape: (451817, 6)\n",
      "Processing split validation\n",
      "source shape: (451593, 6)\n",
      "new source shape: (451593, 6)\n",
      "Processing index: 4\n",
      "Processing split train\n",
      "source shape: (1580762, 6)\n",
      "new source shape: (1580762, 6)\n",
      "Processing split test\n",
      "source shape: (451833, 6)\n",
      "new source shape: (451833, 6)\n",
      "Processing split validation\n",
      "source shape: (225752, 6)\n",
      "new source shape: (225752, 6)\n",
      "Processing index: 5\n",
      "Processing split train\n",
      "source shape: (112830, 6)\n",
      "new source shape: (112830, 6)\n",
      "Processing split test\n",
      "source shape: (112830, 6)\n",
      "new source shape: (112830, 6)\n",
      "Processing split validation\n",
      "source shape: (112830, 6)\n",
      "new source shape: (112830, 6)\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'test', 'validation']\n",
    "dataset_indices = [2,3,4,5]\n",
    "\n",
    "for dataset_index in dataset_indices:\n",
    "    print(f\"Processing index: {dataset_index}\")\n",
    "    for split in splits:\n",
    "        print(f\"Processing split {split}\")\n",
    "        source_df = pd.read_parquet(dataset_path / f\"{split}_df_dataset_{dataset_index}.pq\")\n",
    "        print(f\"source shape: {source_df.shape}\")\n",
    "\n",
    "        source_df = source_df.join(df[['abstract']], rsuffix='_right')\n",
    "        source_df['abstract'] = source_df['abstract_right']\n",
    "        source_df = source_df.drop(columns=['abstract_right'])\n",
    "\n",
    "        target_name = dataset_path / f\"lemmatized_{split}_df_dataset_{dataset_index}.pq\"\n",
    "        print(f\"new source shape: {source_df.shape}\")\n",
    "        source_df.to_parquet(target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4132dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
